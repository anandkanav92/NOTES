
# Lecture 1

- History stuff. Nothing important but good introductions.
- 2012 was the year of alexa net. 
- Neural architecture existed for long time but lack of resources and data made it difficult to predict.
- 2001 adaboost was used for face detection :O 

# Lecture 2
- Talks about KNN, L1, L2 and linear classifiers
- L1 is dependent on your coordinate system. So if values depict some special pre defined meaning, it is good to use L1. L1 has corners while L2 is a sphere.
- L1 summation might not have unique solution always but L2 is always unique. [more details](https://stats.stackexchange.com/questions/363144/why-does-the-l2-norm-loss-have-a-unique-solution-and-the-l1-norm-loss-have-possi)
- Linear classifier - F(x) = Wx + B. output = [10,1] class probabilities, W = [10,32*32*3], x = [32*32*3,1] and b = [10,1]


# Lecture 3
- Hinge loss L = $\frac{\sum L_{i}}{n}$ = $\frac{\sum({S_{wrong} - S_{true} + 1})}{n}$ where 1 is safety margin. 
- Softmax classifier loss L = $\frac{\sum L_i}{n}$ = $-log(\frac{e^{score}}{\sum{e^{all score}}})$ 
- Optimisation of weights to reduce the loss. Done using gradient  descent.
- Stochastic gradient uses mini batches to calculate the approximate loss to decide the direction.

# Lecture 4
- Activation functions: step function, sigmoid, tanh, relu etc etc
- main work of activation function is to activate the neuron based on input. 
- step function does the work but 
	- linear in nature, all layer becomes linear and thus the whole network can be replaces by a linear layer
	- many neurons can have 1s and 0s ( because there is not intermediate)
- Sigmoid:
	- Very popular, limits the activation values (0,1).
	- problem : vanishing gradient. very slow in the middle to make it learn
- tanh:
	- scaled sigmoid.
	- Have stronger gradient than sigmoid but still has vanishing gradient problem
- Relu
	- sigmoid and tanh has analog values. That means each neuron contributes to the activation of next layer. Thus, it becomes computationally expensive. Relu solves this problem by $max(0,x)$. That means it is never negative and thus reduces the density of network by making negative neurons 0.
	- Assigning 0 to neurons give rise to a problem in Relu known as dead gradient. That is, neurons when reach this horizontal line, they dont respont to the input x as gradient is 0. 
	- many versions like leaky rely $y = 0.1x$ are considered so that the gradient isn't 0.

# Pre Processing

- `Mean Subtraction` subtracting mean across every feature in the data. `X -= np.mean(X,axis=0)`
- `Normalization` by centering the mean and dividing the distribution with its standard deviation. `X /= np.std(X,axis=0)`. Normalization is applied when features are of different scale but carries equal weight for the algorithm.
- `PCA` Uses the covariance matrix to identify the top features and rotates the original data using SVD decomposition.
	- the covariance matrix is symmetric and positive semi definite.
	- Dimensionality reduction is done using top values from eigen vector matrix(U)
```python
#Assume input data matrix X of size [N x D]
X -= np.mean(X, axis = 0) # zero-center the data (important)
cov = np.dot(X.T, X) / X.shape[0] # get the data covariance matrix
U,S,V = np.linalg.svd(cov)
Xrot = np.dot(X, U)
Xrot_reduced = np.dot(X, U[:,:100])
Xwhite = Xrot / np.sqrt(S + 1e-5) #Whitening
```
-	`Whitening` The whitening operation takes the data in the eigenbasis and divides every dimension by the eigenvalue to normalize the scale

# Weight initialisation

- `All zeros` Strict NO. All neurons calculate the same output and in turn same gradient and undergo the same parameter updates.
- `small random numbers` Better. `W= 0.01*np.random.randn(D,H)`. One problem, small weight = smaller gradients and it could greatly diminish the gradient.
- `calibrating the weight variance with 1/sqrt(n)` the variance increases with neuron count and thus it can be reduced to 1 by dividing each weight by sqrt(n), where n is the number of neurons. For ReLU, use  `w = np.random.randn(n) * sqrt(2.0/n)`
- `sparse initialization` keep all weights 0 but each neuron is randomly connected to a fixed number of neurons below it.
- `initialise the bias` doesnt make much difference in practice.
- `Batch Normalization`

# Regularization
-	`L2 regularization` penalising the squared magnitude of all parameters. $\frac{1}{2} \lambda w^2$. I


> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTQzNzcyMTAzNCwtMjE2MTA2NTM0LDYyMD
M4MTU2NCwtMTI0ODQyMjkzOCw0OTIzNTYxMTEsNTk2ODMyMjQy
LDg2MTg4OTI5MCwxMTY2OTE2MDMxLC03OTIxMzAxMjMsLTk0OD
g2MzE5OCw5Mjg2NzA2NTMsMTMwNTMzNDU3MSwtNDM4MTg5ODI1
LDIzNTk4MDE1NSwyMTAzNjE0Mzg5XX0=
-->