
# Lecture 1

- History stuff. Nothing important but good introductions.
- 2012 was the year of alexa net. 
- Neural architecture existed for long time but lack of resources and data made it difficult to predict.
- 2001 adaboost was used for face detection :O 

# Lecture 2
- Talks about KNN, L1, L2 and linear classifiers
- L1 is dependent on your coordinate system. So if values depict some special pre defined meaning, it is good to use L1. L1 has corners while L2 is a sphere.
- L1 summation might not have unique solution always but L2 is always unique. [more details](https://stats.stackexchange.com/questions/363144/why-does-the-l2-norm-loss-have-a-unique-solution-and-the-l1-norm-loss-have-possi)
- Linear classifier - F(x) = Wx + B. output = [10,1] class probabilities, W = [10,32*32*3], x = [32*32*3,1] and b = [10,1]


# Lecture 3
- Hinge loss L = $\frac{\sum L_{i}}{n}$ = $\frac{\sum({S_{wrong} - S_{true} + 1})}{n}$ where 1 is safety margin. 
- Softmax classifier loss L = $\frac{\sum L_i}{n}$ = $-log(\frac{e^{score}}{\sum{e^{all score}}})$ 
- Optimisation of weights to reduce the loss. Done using gradient  descent.
- Stochastic gradient uses mini batches to calculate the approximate loss to decide the direction.

# Lecture 4
- Activation functions: step function, sigmoid, tanh, relu etc etc
- main work of activation function is to activate the neuron based on input. 
- step function does the work but 
	- linear in nature, all layer becomes linear and thus the whole network can be replaces by a linear layer
	- many neurons can have 1s and 0s ( because there is not intermediate)
- Sigmoid:
	- Very popular, limits the activation values (0,1).
	- problem : vanishing gradient. very slow in the middle to make it learn
- tanh:
	- scaled sigmoid.
	- Have stronger gradient than sigmoid but still has vanishing gradient problem
- Relu
	- sigmoid and tanh has analog values. That means each neuron contributes to the activation of next layer. Thus, it becomes computationally expensive. Relu solves this problem by $max(0,x)$. That means it is never negative and thus reduces the density of network by making negative neurons 0.
	- Assigning 0 to neurons give rise to a problem in Relu known as dead gradient. That is, neurons when reach this horizontal line, they dont respont to the input x as gradient is 0. 
	- many versions like leaky rely $y = 0.1x$ are considered so that the gradient isn't 0.

# Pre Processing

- `Mean Subtraction` subtracting mean across every feature in the data. `X -= np.mean(X,axis=0)`
- `Normalization` by centering the mean and dividing the distribution with its standard deviation. `X /= np.std(X,axis=0)`. Normalization is applied when features are of different scale but carries equal weight for the algorithm.
- `PCA` Uses the covariance matrix to identify the top features and rotates the original d 
> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTIzMzY2MTY3NCw1OTY4MzIyNDIsODYxOD
g5MjkwLDExNjY5MTYwMzEsLTc5MjEzMDEyMywtOTQ4ODYzMTk4
LDkyODY3MDY1MywxMzA1MzM0NTcxLC00MzgxODk4MjUsMjM1OT
gwMTU1LDIxMDM2MTQzODldfQ==
-->