
# Lecture 1

- History stuff. Nothing important but good introductions.
- 2012 was the year of alexa net. 
- Neural architecture existed for long time but lack of resources and data made it difficult to predict.
- 2001 adaboost was used for face detection :O 

# Lecture 2
- Talks about KNN, L1, L2 and linear classifiers
- L1 is dependent on your coordinate system. So if values depict some special pre defined meaning, it is good to use L1. L1 has corners while L2 is a sphere.
- L1 summation might not have unique solution always but L2 is always unique. [more details](https://stats.stackexchange.com/questions/363144/why-does-the-l2-norm-loss-have-a-unique-solution-and-the-l1-norm-loss-have-possi)
- Linear classifier - F(x) = Wx + B. output = [10,1] class probabilities, W = [10,32*32*3], x = [32*32*3,1] and b = [10,1]


# Lecture 3
- Hinge loss L = $\frac{\sum L_{i}}{n}$ = $\frac{\sum({S_{wrong} - S_{true} + 1})}{n}$ where 1 is safety margin. 
- Softmax classifier loss L = $\frac{\sum L_i}{n}$ = $-log(\frac{e^{score}}{\sum{e^{all score}}})$ 
- Optimisation of weights to reduce the loss. Done using gradient  descent.
- Stochastic gradient uses mini batches to calculate the approximate loss to decide the direction.

# Lecture 4
- Activation functions: step function, sigmoid, tanh, relu etc etc
- main work of activation function is to activate the neuron based on input. 
- step function does the work but 
	- linear in nature, all layer becomes linear and thus the whole network can be replaces by a linear layer
	- many neurons can have 1s and 0s ( because there is not intermediate)
- Sigmoid:
	- Very popular, limits the activation values (0,1).
	- problem : vanishing gradient. very slow in the middle to make it learn
- tanh:
	- scaled sigmoid.
	- Have stronger gradient than sigmoid but still has vanishing gradient problem
- Relu
	- sigmoid and tanh has analog values. That means each neuron contributes to the activation of next layer. Thus, it becomes computationally expensive. Relu solves this problem by $max(0,x)$. That means it is never negative and thus reduces the density of network by making negative neurons 0.
	- Assigning 0 to neurons give rise to a problem in Relu known as dead gradient. That is, neurons when reach this horizontal line, they dont respont to the input x as gradient is 0. 
	- many versions like leaky rely $y = 0.1x$ are considered so that the gradient isn't 0.

# Pre Processing

- `Mean Subtraction` subtracting mean across every feature in the data. `X -= np.mean(X,axis=0)`
- `Normalization` by centering the mean and dividing the distribution with its standard deviation. `X /= np.std(X,axis=0)`. Normalization is applied when features are of different scale but carries equal weight for the algorithm.
- `PCA` Uses the covariance matrix to identify the top features and rotates the original data using SVD decomposition.
	- the covariance matrix is symmetric and positive semi definite.
	- Dimensionality reduction is done using top values from eigen vector matrix(U)
```python
#Assume input data matrix X of size [N x D]
X -= np.mean(X, axis = 0) # zero-center the data (important)
cov = np.dot(X.T, X) / X.shape[0] # get the data covariance matrix
U,S,V = np.linalg.svd(cov)
Xrot = np.dot(X, U)
Xrot_reduced = np.dot(X, U[:,:100])
Xwhite = Xrot / np.sqrt(S + 1e-5) #Whitening
```
-	`Whitening` The whitening operation takes the data in the eigenbasis and divides every dimension by the eigenvalue to normalize the scale

# Weight initialisation

- `All zeros` Strict NO. All neurons calculate the same output and in turn same gradient and undergo the same parameter updates.
- `small random numbers` Better. `W= 0.01*np.random.randn(D,H)`. One problem, small weight = smaller gradients and it could greatly diminish the gradient.
- `calibrating the weight variance with 1/sqrt(n)` the variance increases with neuron count and thus it can be reduced to 1 by dividing each weight by sqrt(n), where n is the number of neurons. For ReLU, use  `w = np.random.randn(n) * sqrt(2.0/n)`
- `sparse initialization` keep all weights 0 but each neuron is randomly connected to a fixed number of neurons below it.
- `initialise the bias` doesnt make much difference in practice.
- `Batch Normalization`

# Regularization
-	`L2 regularization` penalising the squared magnitude of all parameters. $\frac{1}{2} \lambda w^2$.
	-	It heavily penalises the peaky weights. Also notice that GD after adding regularization can be looked as W += -lambda * W.
-	`L1 reglarization`$\lambda |w|$. 
	-	It leads the weight vectors to become sparse i.e many neurons become inactive while final weight vectors are usually small numbers i.e diffuse. 
	-	Elastic net = $\frac{1}{2} \lambda w^2$ + $\lambda w$.
	-	Usually L2>L1
- `Max norm constraints` enforce an absolute upper bound on the weights for each neuron. Typical values of c is of order 3 or 4. It prevents exploding of weights in the network even if learning rate is high as updates are always bounded.
- `Dropout`Keep a neuron active with probability p. It is only applied in training by randomly de-activating few neurons. 
- During testing each weight is scaled 
``` python 
""" Vanilla Dropout: Not recommended implementation (see notes below) """

p = 0.5 # probability of keeping a unit active. higher = less dropout

def train_step(X):
  """ X contains the data """
  
  # forward pass for example 3-layer neural network
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  U1 = np.random.rand(*H1.shape) < p # first dropout mask
  H1 *= U1 # drop!
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  U2 = np.random.rand(*H2.shape) < p # second dropout mask
  H2 *= U2 # drop!
  out = np.dot(W3, H2) + b3
  
  # backward pass: compute gradients... (not shown)
  # perform parameter update... (not shown)
  
def predict(X):
  # ensembled forward pass
  H1 = np.maximum(0, np.dot(W1, X) + b1) * p # NOTE: scale the activations
  H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # NOTE: scale the activations
  out = np.dot(W3, H2) + b3
```


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE2NTQyODMxOTIsLTEwNjg1MDgxNDksMT
kyNDYwMjkxNSwtMjE2MTA2NTM0LDYyMDM4MTU2NCwtMTI0ODQy
MjkzOCw0OTIzNTYxMTEsNTk2ODMyMjQyLDg2MTg4OTI5MCwxMT
Y2OTE2MDMxLC03OTIxMzAxMjMsLTk0ODg2MzE5OCw5Mjg2NzA2
NTMsMTMwNTMzNDU3MSwtNDM4MTg5ODI1LDIzNTk4MDE1NSwyMT
AzNjE0Mzg5XX0=
-->