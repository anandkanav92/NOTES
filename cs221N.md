
# Lecture 1

- History stuff. Nothing important but good introductions.
- 2012 was the year of alexa net. 
- Neural architecture existed for long time but lack of resources and data made it difficult to predict.
- 2001 adaboost was used for face detection :O 

# Lecture 2
- Talks about KNN, L1, L2 and linear classifiers
- L1 is dependent on your coordinate system. So if values depict some special pre defined meaning, it is good to use L1. L1 has corners while L2 is a sphere.
- L1 summation might not have unique solution always but L2 is always unique. [more details](https://stats.stackexchange.com/questions/363144/why-does-the-l2-norm-loss-have-a-unique-solution-and-the-l1-norm-loss-have-possi)
- Linear classifier - F(x) = Wx + B. output = [10,1] class probabilities, W = [10,32*32*3], x = [32*32*3,1] and b = [10,1]


# Lecture 3
- Hinge loss L = $\frac{\sum L_{i}}{n}$ = $\frac{\sum({S_{wrong} - S_{true} + 1})}{n}$ where 1 is safety margin. 
- Softmax classifier loss L = $\frac{\sum L_i}{n}$ = $-log(\frac{e^{score}}{\sum{e^{all score}}})$ 
- Optimisation of weights to reduce the loss. Done using gradient  descent.
- Stochastic gradient uses mini batches to calculate the approximate loss to decide the direction.

# Lecture 4
- Activation functions: step function, sigmoid, tanh, relu etc etc
- main work of activation function is to activate the neuron based on input. 
- step function does the work but 
	- linear in nature, all layer bec
> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTQ5MzU5ODcxMywxMzA1MzM0NTcxLC00Mz
gxODk4MjUsMjM1OTgwMTU1LDIxMDM2MTQzODldfQ==
-->