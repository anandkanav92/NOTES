
# Are all GANs created equal? 
[pdf](https://arxiv.org/pdf/1711.10337.pdf) 12$^{th}$ Dec 2018
## Introduction
## Metric
- Inception score 
	- It offers a way to quantiatively evaluate the quality of samples generated using GAN. Inception network classify the generated images and predict the label for it. This reflects the quality of the image.
	- It has two criterias, saliency and diversity, that the distribution obtained has low entropy i.e samples contain meaningful objects and the samples vary enough to cover all majority of the classes. Saliency is expressed as $p(y|x)$ (think of it as single high score and rest very low). While Diversity, $p(x)$ is overall distribution of the classes across sampled data. (well balanced training set)
	- Finally, inception score is defined as $$ IS(x) = e^{E_x[KL(p(y|x) || p(y)]}$$
	- Kullback-Leibler distance(KL distance)
		- Given two distributions, KL distance find the better fit. 
		- One is observed while other is the approximated version of the distribution. It is not the distance metric because it isn't symmetric.
		- Expressed as, $D_{KL} (Observed || Approximated) = E[log p(x) - log q(x)]$
		- If youâ€™re trying to find approximations for a complex (intractable) distribution ğ‘(ğ‘¥)p(x) by a (tractable) approximate distribution ğ‘(ğ‘¥)q(x) you want to be absolutely sure that any ğ‘¥x that would be very improbable to be drawn from ğ‘(ğ‘¥)p(x) would also be very improbable to be drawn from ğ‘(ğ‘¥)q(x). That KL have this property is easily shown: thereâ€™s a ğ‘(ğ‘¥)ğ‘™ğ‘œğ‘”[ğ‘(ğ‘¥)/ğ‘(ğ‘¥)]q(x)log[q(x)/p(x)] in the integrand. When ğ‘(ğ‘¥)q(x) is small but ğ‘(ğ‘¥)p(x) is not, thatâ€™s ok. But when ğ‘(ğ‘¥)p(x)is small, this grows very rapidly if ğ‘(ğ‘¥)q(x) isnâ€™t also small. So, if youâ€™re choosing ğ‘(ğ‘¥)q(x) to minimize ğ¾ğ¿[ğ‘;ğ‘]KL[q;p], itâ€™s very improbable that ğ‘(ğ‘¥)q(x) will assign a lot of mass on regions where ğ‘(ğ‘¥)p(x) is near zero.
		- If p(x) has x point where the distribution is very low, the impact of q(x) is minimized if it is larger. Similary, points where q(x) has low probability, it doesn't impact the overall distance from learning.
- FrÃ©chet Inception Distance(FID)
	- Features are extracted from an intermediate layer of Inception Net. The data is fit on multivariate gaussian distribution and FDI is calculated b/w generated data and real data using $$ FID(x,g) = ||mean_x - mean_g||^2 + Tr(\sum_x + \sum_g - 2(\sum_x \sum_g)^\frac{1}{2})$$
	- results show robust nature of FDI, can detect intra class mode dropping i.e model that generates only one image per class can score high IS but not FID.
	- Inability to detect overfitting
- Precision, Recall and F1
	- If the generated images look very similar to real images on avg, high precision. $$ P_{recision} = \frac{tp}{tp+fp}$$
	- If the model can generate any sample found in the training set, high recall. $$ R_{ecall} = \frac{tp}{tp+fn}$$
	- Harmonic mean of precision and recall = F1 score. $$ F1 = 2 \frac{P R}{P+R}$$ 
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTY2MjUwMzUwMywxMjM4ODc2NTAyLDE0OD
EzMjUwNTcsLTk5MDIzNTMzMiwtMTgyODQ4Mjc3NywtMTI3MTE3
Nzg2LC0zNzkxNTU5MTEsLTUwOTkyODc4Myw5MzIyNDUxMTksLT
E1NzIzMDEyMjcsLTI3MTUzNjUxNl19
-->